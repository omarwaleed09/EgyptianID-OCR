{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf17f0c-fb98-4062-86be-75f38ad895c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import numpy as np\n",
    "import mapping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import splitfolders\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73dda7e8-d9e2-449a-b1cb-567346f38447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2884 files belonging to 2 classes.\n",
      "Found 780 files belonging to 2 classes.\n",
      "Found 457 files belonging to 2 classes.\n",
      "Classes: ['female', 'male']\n"
     ]
    }
   ],
   "source": [
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "gender_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"..\\data\\dataset_cnn_2\\data\\gender\\train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "gender_val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"..\\data\\dataset_cnn_2\\data\\gender\\val\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "gender_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"..\\data\\dataset_cnn_2\\data\\gender\\test\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "class_names_gender = gender_train_ds.class_names\n",
    "print(\"Classes:\", class_names_gender)\n",
    "num_classes = len(class_names_gender)\n",
    "\n",
    "gender_train_ds = gender_train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "gender_val_ds = gender_val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "gender_test_ds = gender_test_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "696143f9-66a1-4f9e-aae3-237fb1ab053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_gender = {\n",
    "    \"male\": \"ذكر\",\n",
    "    \"female\": \"أنثى\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bbd86f-1b20-463d-8be4-50a6b4fb596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    \n",
    "    layers.Input(shape=(64, 64, 3)),\n",
    "\n",
    "    layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Conv2D(64, 3, activation='relu',padding='same'),\n",
    "    layers.Conv2D(64, 3, activation='relu',padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    #layers.AveragePooling2D(pool_size=(2,2)),#MinPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    layers.Conv2D(128, 3, activation='relu',padding='same'),\n",
    "    layers.Conv2D(128, 3, activation='relu',padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    #layers.MaxPooling2D(),\n",
    "    #layers.Conv2D(256, 3, activation='relu'),\n",
    "    #layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    #MinPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba57cc39-b095-403f-a291-d8198a88ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b7b184-ac9b-4ad6-8216-878bda4a2bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 270ms/step - accuracy: 0.6300 - loss: 0.6001 - val_accuracy: 0.9051 - val_loss: 0.2966\n",
      "Epoch 2/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 275ms/step - accuracy: 0.9771 - loss: 0.0702 - val_accuracy: 0.9974 - val_loss: 0.0098\n",
      "Epoch 3/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 284ms/step - accuracy: 0.9965 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 4/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 267ms/step - accuracy: 0.9997 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 5.0073e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 263ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.2124e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 263ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 5.4659e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 269ms/step - accuracy: 0.9993 - loss: 8.8230e-04 - val_accuracy: 1.0000 - val_loss: 1.5081e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 264ms/step - accuracy: 0.9997 - loss: 8.7624e-04 - val_accuracy: 1.0000 - val_loss: 2.7931e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 270ms/step - accuracy: 1.0000 - loss: 2.0016e-04 - val_accuracy: 1.0000 - val_loss: 3.7711e-06\n",
      "Epoch 10/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 271ms/step - accuracy: 0.9997 - loss: 8.8562e-04 - val_accuracy: 1.0000 - val_loss: 2.3568e-06\n",
      "Epoch 11/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 265ms/step - accuracy: 1.0000 - loss: 4.6879e-04 - val_accuracy: 1.0000 - val_loss: 6.7361e-07\n",
      "Epoch 12/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 266ms/step - accuracy: 1.0000 - loss: 5.4180e-04 - val_accuracy: 1.0000 - val_loss: 4.6553e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 266ms/step - accuracy: 0.9899 - loss: 0.0285 - val_accuracy: 1.0000 - val_loss: 3.3166e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 269ms/step - accuracy: 0.9976 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 4.6808e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 258ms/step - accuracy: 0.9976 - loss: 0.0109 - val_accuracy: 1.0000 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(gender_train_ds, epochs=15, \n",
    "                    validation_data=gender_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33514aff-cd9c-421c-8171-5add0f64ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9978 - loss: 0.0079\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(gender_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1853a4ca-5fa4-47e7-9807-4606a61a15bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/predictions_CNN_muslim.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"..\\data\\dataset_cnn_2\\dataset\\gender\\female\"\n",
    "#D:\\ocr_project\\data\\dataset_cnn_2\\output_img\\result\\crops\\gender\\male\n",
    "#religion\\muslim_m   \"D:\\ocr_project\\data\\dataset_cnn_2\\output_img\\result\\crops\\gender\\male\"\n",
    "#marital status\\widower   \"..\\data\\dataset_cnn_2\\data\\test\\male\"\n",
    "output_file = \"../outputs/predictions_CNN_muslim.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "       \n",
    "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "      \n",
    "        img = image.load_img(img_path, target_size=(64,64))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = img_array / 255.0\n",
    "\n",
    "     \n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "        en_label = class_names_gender[predicted_label]\n",
    "        ar_label = label_map_gender.get(en_label, en_label)\n",
    "\n",
    "     \n",
    "        f.write(f\"{img_name}: {ar_label}\\n\")\n",
    "\n",
    "print(f\"{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dfd413a-7d22-4ae0-a5ee-9ce5696a8e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/predictions_CNN_muslim.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"..\\data\\dataset_cnn_2\\dataset\\gender\\female\"\n",
    "#D:\\ocr_project\\data\\dataset_cnn_2\\output_img\\result\\crops\\gender\\male\n",
    "#religion\\muslim_m   \"D:\\ocr_project\\data\\dataset_cnn_2\\output_img\\result\\crops\\gender\\male\"\n",
    "#marital status\\widower   \"..\\data\\dataset_cnn_2\\data\\test\\male\"\n",
    "output_file = \"../outputs/predictions_CNN_muslim.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "       \n",
    "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "      \n",
    "        img = image.load_img(img_path, target_size=(64,64))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = img_array / 255.0\n",
    "\n",
    "     \n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "        en_label = class_names_gender[predicted_label]\n",
    "        ar_label = label_map_gender.get(en_label, en_label)\n",
    "\n",
    "     \n",
    "        f.write(f\"{img_name}: {ar_label}\\n\")\n",
    "\n",
    "print(f\"{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0142ee-763f-483f-9a03-04133baf50df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
