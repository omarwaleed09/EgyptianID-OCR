{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4c8817-d5fb-4440-be00-55bd808159b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import numpy as np\n",
    "import mapping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import splitfolders\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84eeb809-edb3-4667-a56f-12c7062f406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_gender = {\n",
    "    \"male\": \"ذكر\",\n",
    "    \"female\": \"أنثى\",\n",
    "}\n",
    "label_map_religion= {\n",
    "    \"muslim_m\": \"مسلم\",\n",
    "    \"muslim_f\": \"مسلمة\",\n",
    "    \"christian_m\": \"مسيحي\",\n",
    "    \"christian_f\": \"مسيحية\"\n",
    "}\n",
    "label_map_marital_status={\n",
    "    \"single_m\": \"أعزب\",\n",
    "    \"single_f\": \"أنسة\",\n",
    "    \"married_m\": \"متزوج\",\n",
    "    \"married_f\": \"متزوجة\",\n",
    "    \"widow\": \"أرملة\",\n",
    "    \"widower\": \"أرمل\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511dd67b-dfdf-4635-bf52-05595bab5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2884 files belonging to 2 classes.\n",
      "Found 780 files belonging to 2 classes.\n",
      "Found 457 files belonging to 2 classes.\n",
      "Classes: ['female', 'male']\n"
     ]
    }
   ],
   "source": [
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "\n",
    "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     r\"..\\data\\dataset_cnn_2\\data\\train\",\n",
    "#     seed=123,\n",
    "#     image_size=img_size,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size,\n",
    "#     color_mode=\"rgb\"  \n",
    "# )\n",
    "\n",
    "# val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     r\"..\\data\\dataset_cnn_2\\data\\val\",\n",
    "#     seed=123,\n",
    "#     shuffle=True,\n",
    "#     image_size=img_size,\n",
    "#     batch_size=batch_size,\n",
    "#     color_mode=\"rgb\"\n",
    "# )\n",
    "\n",
    "# test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     r\"..\\data\\dataset_cnn_2\\data\\test\",\n",
    "#     seed=123,\n",
    "#     shuffle=True,\n",
    "#     image_size=img_size,\n",
    "#     batch_size=batch_size,\n",
    "#     color_mode=\"rgb\"\n",
    "# )\n",
    "\n",
    "gender_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"..\\data\\dataset_cnn_2\\data\\gender\\train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "gender_val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"..\\data\\dataset_cnn_2\\data\\gender\\val\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "gender_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"..\\data\\dataset_cnn_2\\data\\gender\\test\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "class_names_gender = gender_train_ds.class_names\n",
    "print(\"Classes:\", class_names_gender)\n",
    "num_classes = len(class_names_gender)\n",
    "\n",
    "gender_train_ds = gender_train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "gender_val_ds = gender_val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "gender_test_ds = gender_test_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5dadf7d-3058-4867-99f6-7824b6e501eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # class MinPooling2D(layers.Layer):\n",
    "# #     def __init__(self, pool_size=(2, 2), strides=None, padding=\"VALID\", **kwargs):\n",
    "# #         super(MinPooling2D, self).__init__(**kwargs)\n",
    "# #         self.pool_size = pool_size\n",
    "# #         self.strides = strides if strides is not None else pool_size\n",
    "# #         self.padding = padding\n",
    "\n",
    "# #     def call(self, inputs):\n",
    "# #         return -tf.nn.max_pool2d(\n",
    "# #             -inputs,\n",
    "# #             ksize=self.pool_size,\n",
    "# #             strides=self.strides,\n",
    "# #             padding=self.padding\n",
    "# #         )\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from keras import layers\n",
    "# from keras.saving import register_keras_serializable\n",
    "\n",
    "# @register_keras_serializable(package=\"Custom\")\n",
    "# class MinPooling2D(layers.Layer):\n",
    "#     def __init__(self, pool_size=(2, 2), strides=None, padding=\"VALID\", **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.pool_size = tuple(pool_size)\n",
    "#         self.strides = tuple(strides) if strides is not None else tuple(pool_size)\n",
    "#         self.padding = padding.upper()\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         return -tf.nn.max_pool2d(\n",
    "#             -inputs,\n",
    "#             ksize=self.pool_size,\n",
    "#             strides=self.strides,\n",
    "#             padding=self.padding\n",
    "#         )\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             \"pool_size\": self.pool_size,\n",
    "#             \"strides\": self.strides,\n",
    "#             \"padding\": self.padding\n",
    "#         })\n",
    "#         return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6d9fcf-66c4-4b0a-9466-1c70937f4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers.MinPooling import MinPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca36cb44-3383-4419-8eb9-1c4be03556aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    #layers.Conv2D(16, 3, activation='relu', input_shape=(64, 64, 3)),\n",
    "    #layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    MinPooling2D(pool_size=(2,2)),\n",
    "    #layers.MaxPooling2D(),\n",
    "    #layers.AveragePooling2D(pool_size=(2,2)),\n",
    "\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    #layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Conv2D(256, 3, activation='relu'),\n",
    "    #layers.AveragePooling2D(pool_size=(2,2)),\n",
    "    MinPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4783ceb2-2573-41f3-ab3f-e15221294626",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d6c6c4-75d0-4258-9301-3930adf9deff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 480ms/step - accuracy: 0.8499 - loss: 0.2805 - val_accuracy: 0.9987 - val_loss: 0.0065\n",
      "Epoch 2/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 455ms/step - accuracy: 0.9951 - loss: 0.0192 - val_accuracy: 0.9949 - val_loss: 0.0159\n",
      "Epoch 3/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 487ms/step - accuracy: 0.9976 - loss: 0.0117 - val_accuracy: 0.9923 - val_loss: 0.0169\n",
      "Epoch 4/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 443ms/step - accuracy: 0.9972 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 5/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 293ms/step - accuracy: 0.9927 - loss: 0.0231 - val_accuracy: 0.9167 - val_loss: 0.2036\n",
      "Epoch 6/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 607ms/step - accuracy: 0.9893 - loss: 0.0410 - val_accuracy: 0.9974 - val_loss: 0.0033\n",
      "Epoch 7/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 619ms/step - accuracy: 0.9969 - loss: 0.0096 - val_accuracy: 0.9987 - val_loss: 0.0029\n",
      "Epoch 8/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 516ms/step - accuracy: 0.9976 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 8.0811e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 608ms/step - accuracy: 0.9979 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 8.1428e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 616ms/step - accuracy: 0.9990 - loss: 0.0023 - val_accuracy: 0.9987 - val_loss: 0.0020\n",
      "Epoch 11/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 617ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 0.9987 - val_loss: 0.0022\n",
      "Epoch 12/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 621ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 8.6672e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 579ms/step - accuracy: 0.9997 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 8.8210e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 585ms/step - accuracy: 0.9990 - loss: 0.0047 - val_accuracy: 0.9962 - val_loss: 0.0200\n",
      "Epoch 15/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 529ms/step - accuracy: 0.9941 - loss: 0.0183 - val_accuracy: 0.9833 - val_loss: 0.0553\n",
      "Epoch 16/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 494ms/step - accuracy: 0.9983 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 5.0517e-05\n",
      "Epoch 17/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 583ms/step - accuracy: 0.9979 - loss: 0.0076 - val_accuracy: 0.9987 - val_loss: 0.0054\n",
      "Epoch 18/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 585ms/step - accuracy: 0.9976 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 6.5253e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 595ms/step - accuracy: 0.9983 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 20/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 506ms/step - accuracy: 0.9979 - loss: 0.0038 - val_accuracy: 0.9987 - val_loss: 0.0024\n",
      "Epoch 21/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 544ms/step - accuracy: 0.9993 - loss: 0.0019 - val_accuracy: 0.9987 - val_loss: 0.0024\n",
      "Epoch 22/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 579ms/step - accuracy: 0.9997 - loss: 8.9521e-04 - val_accuracy: 1.0000 - val_loss: 6.7399e-05\n",
      "Epoch 23/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 587ms/step - accuracy: 1.0000 - loss: 2.8200e-04 - val_accuracy: 1.0000 - val_loss: 9.7436e-05\n",
      "Epoch 24/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 403ms/step - accuracy: 1.0000 - loss: 1.3561e-04 - val_accuracy: 1.0000 - val_loss: 2.2588e-05\n",
      "Epoch 25/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 392ms/step - accuracy: 1.0000 - loss: 1.7013e-04 - val_accuracy: 1.0000 - val_loss: 2.0078e-05\n",
      "Epoch 26/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 397ms/step - accuracy: 1.0000 - loss: 7.1788e-05 - val_accuracy: 1.0000 - val_loss: 3.9870e-06\n",
      "Epoch 27/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 347ms/step - accuracy: 1.0000 - loss: 5.1188e-05 - val_accuracy: 1.0000 - val_loss: 5.1106e-06\n",
      "Epoch 28/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 389ms/step - accuracy: 1.0000 - loss: 3.3082e-05 - val_accuracy: 1.0000 - val_loss: 2.5694e-06\n",
      "Epoch 29/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 402ms/step - accuracy: 1.0000 - loss: 2.0110e-05 - val_accuracy: 1.0000 - val_loss: 2.2404e-06\n",
      "Epoch 30/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 410ms/step - accuracy: 1.0000 - loss: 1.4069e-05 - val_accuracy: 1.0000 - val_loss: 2.0199e-06\n",
      "Epoch 31/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 1.0997e-05 - val_accuracy: 1.0000 - val_loss: 1.7510e-06\n",
      "Epoch 32/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 430ms/step - accuracy: 1.0000 - loss: 9.1208e-06 - val_accuracy: 1.0000 - val_loss: 1.5843e-06\n",
      "Epoch 33/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 405ms/step - accuracy: 1.0000 - loss: 8.0192e-06 - val_accuracy: 1.0000 - val_loss: 1.4876e-06\n",
      "Epoch 34/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 394ms/step - accuracy: 1.0000 - loss: 7.1253e-06 - val_accuracy: 1.0000 - val_loss: 1.4136e-06\n",
      "Epoch 35/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 394ms/step - accuracy: 1.0000 - loss: 6.6779e-06 - val_accuracy: 1.0000 - val_loss: 1.0694e-06\n",
      "Epoch 36/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 344ms/step - accuracy: 1.0000 - loss: 5.1730e-06 - val_accuracy: 1.0000 - val_loss: 9.5610e-07\n",
      "Epoch 37/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 399ms/step - accuracy: 1.0000 - loss: 4.0005e-06 - val_accuracy: 1.0000 - val_loss: 1.0812e-06\n",
      "Epoch 38/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 389ms/step - accuracy: 1.0000 - loss: 3.9315e-06 - val_accuracy: 1.0000 - val_loss: 9.0753e-07\n",
      "Epoch 39/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 338ms/step - accuracy: 1.0000 - loss: 3.5438e-06 - val_accuracy: 1.0000 - val_loss: 9.9627e-07\n",
      "Epoch 40/40\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 391ms/step - accuracy: 1.0000 - loss: 2.9535e-06 - val_accuracy: 1.0000 - val_loss: 8.3559e-07\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(gender_train_ds, epochs=40, \n",
    "                    validation_data=gender_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65cf2417-face-48fd-95e0-b27e85c05687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - accuracy: 0.9978 - loss: 0.0182\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(gender_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff7a5ce6-e38d-42d9-a033-b624142da306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/predictions_CNN_muslim.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"..\\notebooks\\outputs\\new\"\n",
    "#D:\\ocr_project\\data\\dataset_cnn_2\\output_img\\result\\crops\\gender\\male\n",
    "#religion\\muslim_m   \"D:\\ocr_project\\data\\dataset_cnn_2\\output_img\\result\\crops\\gender\\male\"\n",
    "#marital status\\widower   \"..\\data\\dataset_cnn_2\\data\\test\\male\"\n",
    "output_file = \"../outputs/predictions_CNN_muslim.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "       \n",
    "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "      \n",
    "        img = image.load_img(img_path, target_size=(64,64))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = img_array / 255.0\n",
    "\n",
    "     \n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "        en_label = class_names[predicted_label]\n",
    "        ar_label = label_map_gender.get(en_label, en_label)\n",
    "\n",
    "     \n",
    "        f.write(f\"{img_name}: {ar_label}\\n\")\n",
    "\n",
    "print(f\"{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf867fae-f82b-4816-a80d-277849be78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/cnn_model_gender.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
